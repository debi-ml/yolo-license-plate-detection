{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPreDwBvqnwk"
      },
      "source": [
        "# License Plate Detection Model Using YOLOv8\n",
        "\n",
        "This notebook presents a **License Plate Detection Model** that utilizes the **YOLOv8** (You Only Look Once version 8) model. YOLOv8 is a state-of-the-art, real-time object detection model that is incredibly effective at identifying objects in images and video.\n",
        "\n",
        "The model has been fine-tuned on the [Car License Plate Detection](https://www.kaggle.com/datasets/andrewmvd/car-plate-detection) dataset from Kaggle. This dataset provides a diverse set of images with annotated license plates, which is ideal for training our model.\n",
        "\n",
        "This work also features the use of **Optical Character Recognition** (OCR) to extract and interpret the content of the detected license plates. This enhancement is achieved through the incorporation of two OCR frameworks: **pytesseract** and **EasyOCR**. These OCRs significantly augment the functionality of the license plate detection model, enabling the extraction of textual information from the predicted bounding boxes.\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "The Car License Plate Detection dataset is a rich collection of car images with annotated license plates. The annotations provide the bounding box coordinates of license plates, making it a valuable resource for training object detection models.\n",
        "\n",
        "## Model Overview\n",
        "\n",
        "The YOLOv8 algorithm divides the input image into an SxS grid and for each grid cell predicts multiple bounding boxes and class probabilities. The bounding boxes are weighted by the predicted probabilities.\n",
        "\n",
        "Our model has been fine-tuned on the aforementioned dataset for the specific task of license plate detection. Fine-tuning involves training the model on a specific dataset after it has been pre-trained on a larger dataset, allowing the model to adapt to new data.\n",
        "\n",
        "## Usage\n",
        "\n",
        "This model can be used in various applications such as traffic surveillance, parking management, and in automated systems where vehicle identification is required.\n",
        "\n",
        "Please note that the use of this model should comply with local laws and regulations related to privacy and data protection.\n",
        "\n",
        "## Future Work\n",
        "\n",
        "While the fine-tuned YOLO model is showing great performance, the next step is enhancing the license plate OCR process. By improving the pre-OCR processing of cropped license plate images for better results. This tweak aims to make text extraction smoother, boosting accuracy and efficiency.\n",
        "\n",
        "## Acknowledgment\n",
        "The implementation presented in this notebook draws significant inspiration from the work shared in this [notebook](https://www.kaggle.com/code/aslanahmedov/automatic-number-plate-recognition) by Aslan Ahmedov.\n",
        "\n",
        "We hope you find this notebook useful for your license plate detection tasks! Happy coding! üöóüîç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE8coSJhex3e"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "\n",
        "# Install Tesseract OCR\n",
        "!apt install -y tesseract-ocr\n",
        "\n",
        "# Set preferred encoding to UTF-8\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale=True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# Install required Python packages\n",
        "!pip install pytesseract easyocr imutils onnxruntime ultralytics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F-PkP8xfUOg"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "from glob import glob\n",
        "from shutil import copy\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from skimage import io\n",
        "import PIL\n",
        "import pytesseract as pt\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import tensorflow as tf\n",
        "import plotly.express as px\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GBH4y_irafq"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "To train and fine-tune the YOLO model, we need a suitable dataset. In this project, we utilize the [Car License Plate Detection dataset](https://www.kaggle.com/datasets/andrewmvd/car-plate-detection) from Kaggle. The dataset provides a diverse collection of images annotated with car license plate information.\n",
        "\n",
        "## Kaggle Dataset\n",
        "\n",
        "1. Visit the [Car License Plate Detection Kaggle dataset page](https://www.kaggle.com/datasets/andrewmvd/car-plate-detection).\n",
        "2. Download the dataset in a zip file format.\n",
        "\n",
        "## Google Drive\n",
        "\n",
        "Once the dataset is obtained, it needs to be upload it to Google Drive for convenient access.\n",
        "\n",
        "Now, we are ready to proceed with the data extraction and exploration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNjq9LC2sssj",
        "outputId": "5cca9c3c-52a4-486f-93cb-027bd2df486b"
      },
      "outputs": [],
      "source": [
        "# Unzip the uploaded file\n",
        "!unzip /content/drive/MyDrive/Licence_plate_data/archive.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXwIYj98ULz3"
      },
      "source": [
        "## Parsing Data from XML Files\n",
        "\n",
        "The downloaded dataset is structured in XML format, with each annotation file providing information about the images, including object labels and bounding box coordinates. Here is an example of the XML file structure:\n",
        "\n",
        "```xml\n",
        "<annotation>\n",
        "    <folder>images</folder>\n",
        "    <filename>Cars0.png</filename>\n",
        "    <size>\n",
        "        <width>500</width>\n",
        "        <height>268</height>\n",
        "        <depth>3</depth>\n",
        "    </size>\n",
        "    <segmented>0</segmented>\n",
        "    <object>\n",
        "        <name>licence</name>\n",
        "        <pose>Unspecified</pose>\n",
        "        <truncated>0</truncated>\n",
        "        <occluded>0</occluded>\n",
        "        <difficult>0</difficult>\n",
        "        <bndbox>\n",
        "            <xmin>226</xmin>\n",
        "            <ymin>125</ymin>\n",
        "            <xmax>419</xmax>\n",
        "            <ymax>173</ymax>\n",
        "        </bndbox>\n",
        "    </object>\n",
        "</annotation>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAnm9Xz1ZMuH"
      },
      "outputs": [],
      "source": [
        "# Parse XML annotations for object detection\n",
        "# Extract relevant information about bounding boxes and image properties\n",
        "# Organize the information into a structured dictionary\n",
        "\n",
        "\n",
        "path = glob(\"/content/annotations/*.xml\") # get a list of file paths for XML files in the /content/annotations/ directory.\n",
        "labels_dict = dict(filepath=[], xmin=[], xmax=[], ymin=[], ymax=[], filename=[], width=[], height=[]) # create labels dictionary\n",
        "\n",
        "# iterate over xml files\n",
        "for file in path:\n",
        "  info = ET.parse(file)\n",
        "  root = info.getroot()\n",
        "  member_object = root.find(\"object\") # get object element\n",
        "  labels_info = member_object.find(\"bndbox\") #get bndbox element\n",
        "\n",
        "  # Extract int values of bounding box dimensions\n",
        "  xmin = int(labels_info.find(\"xmin\").text)\n",
        "  xmax = int(labels_info.find(\"xmax\").text)\n",
        "  ymin = int(labels_info.find(\"ymin\").text)\n",
        "  ymax = int(labels_info.find(\"ymax\").text)\n",
        "\n",
        "  # Extract str values for image filename\n",
        "  filename = root.find(\"filename\").text # get filename element\n",
        "\n",
        "\n",
        "  size_element = root.find(\"size\") # get size element\n",
        "  # Extract int values for image size\n",
        "  width = int(size_element.find(\"width\").text)\n",
        "  height = int(size_element.find(\"height\").text)\n",
        "\n",
        "\n",
        "  # Save the information in the labels_dict\n",
        "  labels_dict[\"filepath\"].append(file)\n",
        "  labels_dict[\"xmin\"].append(xmin)\n",
        "  labels_dict[\"xmax\"].append(xmax)\n",
        "  labels_dict[\"ymin\"].append(ymin)\n",
        "  labels_dict[\"ymax\"].append(ymax)\n",
        "  labels_dict[\"filename\"].append(filename)\n",
        "  labels_dict[\"width\"].append(width)\n",
        "  labels_dict[\"height\"].append(height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ElUL3v35YR14",
        "outputId": "76cf3235-7e18-40e0-ed26-279a73f0fbc9"
      },
      "outputs": [],
      "source": [
        "# Convert this dictionary as pandas dataframe\n",
        "df = pd.DataFrame(labels_dict)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgu1LE5f0Zzu"
      },
      "source": [
        "## YOLO Model Requirements for Bounding Box Representation\n",
        "\n",
        "The transformation of bounding box coordinates in our dataset is necessary to align with the requirements of the YOLO model. YOLO expects bounding box labels to be represented in terms of the center coordinates (X, Y) and the width (W) and height (H) of the bounding box, all normalized to the dimensions of the image.\n",
        "\n",
        "<img src= \"https://github.com/Asikpalysik/Automatic-License-Plate-Detection/blob/main/Presentation/Notebook9.png?raw=true\" width=\"100%\" align=\"center\"  hspace=\"5%\" vspace=\"5%\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wtGS7paWZ_7V",
        "outputId": "8adfa414-0d78-4698-cc6d-edb69de1bdc0"
      },
      "outputs": [],
      "source": [
        "# Center coordinates of the bounding box\n",
        "\n",
        "df[\"x\"] = (df['xmax'] + df['xmin'])/(2*df['width'])\n",
        "df[\"y\"] = (df['ymax'] + df['ymin'])/(2*df['height'])\n",
        "\n",
        "df[\"w\"] = (df['xmax'] - df['xmin'])/df['width']\n",
        "df[\"h\"] = (df['ymax'] - df['ymin'])/df['height']\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPQHykPc1XE7"
      },
      "source": [
        "Let's visualize the image and draw the bounding box in order to verify the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "EEKhrNaC1Sxf",
        "outputId": "2bea42d4-d6d4-498f-deef-42ce2e5b4e30"
      },
      "outputs": [],
      "source": [
        "image_path = \"/content/images/Cars0.png\" #path of our image N2.jpeg\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from PIL import Image\n",
        "\n",
        "#display the image\n",
        "plt.imshow(Image.open(image_path))\n",
        "\n",
        "#add rectangle\n",
        "plt.gca().add_patch(Rectangle((226,125),-(419-226),-(173-125),\n",
        "                    angle=-180,\n",
        "                    edgecolor='red',\n",
        "                   facecolor='none',\n",
        "                    lw=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1cnjq-h9VjW"
      },
      "source": [
        "## Folder Structure for YOLO Models\n",
        "\n",
        "To follow the requirements of YOLO models, we organize our data in the following folder structure:\n",
        "\n",
        "```plaintext\n",
        "data/\n",
        "|-- train/\n",
        "|   |-- image1.jpg\n",
        "|   |-- image1.txt\n",
        "|   |-- image2.jpg\n",
        "|   |-- image2.txt\n",
        "|   |-- ...\n",
        "|\n",
        "|-- test/\n",
        "|   |-- image1.jpg\n",
        "|   |-- image1.txt\n",
        "|   |-- image2.jpg\n",
        "|   |-- image2.txt\n",
        "|   |-- ...\n",
        "```\n",
        "\n",
        "**Training and Test Data**: Images are divided into training and test sets\n",
        "\n",
        "**Label Information:** Corresponding labels are stored in .txt files, sharing the same filenames as the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8IBR27Q-Jq8"
      },
      "outputs": [],
      "source": [
        "# Create directory structure for dataset\n",
        "!mkdir -p /content/datasets/data_images/train /content/datasets/data_images/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaJT9eeH-wkl"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test set\n",
        "df_train = df.iloc[:int(0.8*len(df))]\n",
        "df_test = df.iloc[int(0.8*len(df)):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEEtMRaE_TmH"
      },
      "outputs": [],
      "source": [
        "train_folder = \"/content/datasets/data_images/train\"\n",
        "\n",
        "values = df_train[['filename','x','y','w','h']].values\n",
        "for fname, x,y, w, h in values:\n",
        "    image_name = os.path.split(fname)[-1]\n",
        "    txt_name = os.path.splitext(image_name)[0]\n",
        "\n",
        "    dst_image_path = os.path.join(train_folder,image_name)\n",
        "    dst_label_file = os.path.join(train_folder,txt_name+'.txt')\n",
        "\n",
        "    # copy each image into the folder\n",
        "    copy(\"/content/images/\"+image_name,dst_image_path)\n",
        "\n",
        "    #generate .txt which has label info\n",
        "    label_txt = f'0 {x} {y} {w} {h}'\n",
        "    with open(dst_label_file,mode='w') as f:\n",
        "        f.write(label_txt)\n",
        "        f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6MIFTLDADtk"
      },
      "outputs": [],
      "source": [
        "test_folder = \"/content/datasets/data_images/test\"\n",
        "\n",
        "values = df_test[['filename','x','y','w','h']].values\n",
        "for fname, x,y, w, h in values:\n",
        "    image_name = os.path.split(fname)[-1]\n",
        "    txt_name = os.path.splitext(image_name)[0]\n",
        "\n",
        "    dst_image_path = os.path.join(test_folder,image_name)\n",
        "    dst_label_file = os.path.join(test_folder,txt_name+'.txt')\n",
        "\n",
        "    # copy each image into the folder\n",
        "    copy(\"/content/images/\"+image_name,dst_image_path)\n",
        "\n",
        "    #generate .txt which has label info\n",
        "    label_txt = f'0 {x} {y} {w} {h}'\n",
        "    with open(dst_label_file,mode='w') as f:\n",
        "        f.write(label_txt)\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qsq_mPZNDdDw",
        "outputId": "aeef99b7-039a-47bb-9f85-1510fff3dbf5"
      },
      "outputs": [],
      "source": [
        "# Create data.yaml file\n",
        "# Create and write data.yaml file\n",
        "data_yaml_content = \"\"\"\n",
        "train: data_images/train\n",
        "val: data_images/test\n",
        "nc: 1\n",
        "names: [\n",
        "    'license_plate'\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "with open('data.yaml', 'w') as file:\n",
        "    file.write(data_yaml_content)\n",
        "\n",
        "# Check if the file is created successfully\n",
        "!cat data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNoCN28cC6La"
      },
      "source": [
        "# YOLO Model Initialization and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdXBblDfDCEO",
        "outputId": "c474d482-2938-4d19-bb54-8bf79416a0bc"
      },
      "outputs": [],
      "source": [
        "# Load a model\n",
        "model = YOLO('yolov8n.yaml')  # build a new model from YAML\n",
        "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
        "model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data='data.yaml', epochs=100, device=\"0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV4vPnwyzjjz"
      },
      "source": [
        "## Export as onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "vJ_sElsRFRdX",
        "outputId": "6c3fc222-835d-47da-8004-86f8e52b776b"
      },
      "outputs": [],
      "source": [
        "# Export the model as onnx\n",
        "model.export(format='onnx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fQSlVFGLhVe",
        "outputId": "6d5fcfc1-74e2-4406-dba8-5efbcc47ea6c"
      },
      "outputs": [],
      "source": [
        "# Run inference on 'bus.jpg' with arguments\n",
        "model.predict('/content/car_plate_test.jpg', save=True, conf=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "4sFprOHIMMck",
        "outputId": "deb27f3f-931c-454d-d9ef-bf815905effa"
      },
      "outputs": [],
      "source": [
        "# Set image size setting\n",
        "IMAGE_WIDTH = 640\n",
        "IMAGE_HEIGHT = 640\n",
        "\n",
        "# load sample test image\n",
        "img = io.imread(\"/content/car_plate_test.jpg\")\n",
        "\n",
        "fig = px.imshow(img)\n",
        "fig.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HT9gVu8Q40I"
      },
      "outputs": [],
      "source": [
        "# LOAD YOLO MODEL\n",
        "net = cv2.dnn.readNetFromONNX('/content/drive/MyDrive/Licence_plate_detection_model/runs/detect/train2/weights/best.onnx')\n",
        "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFf5t3h70MYq"
      },
      "source": [
        "## Load the saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xScdGpAAXt1z",
        "outputId": "573830f3-c12f-4782-a1c4-123b46c6ee46"
      },
      "outputs": [],
      "source": [
        " #Load a model\n",
        "loaded_model = YOLO('/content/drive/MyDrive/Licence_plate_detection_model/runs/detect/train2/weights/best.pt')  # load a partially trained model\n",
        "\n",
        "# Run inference on 'bus.jpg' with arguments\n",
        "loaded_model.predict('/content/car_plate_test.jpg', save=True, conf=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "g50RYNROduT8",
        "outputId": "94f1d6fd-dbc3-48cb-9ef8-17cebbc77240"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "# Constants\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Licence_plate_detection_model/runs/detect/train2/weights/best.onnx\"\n",
        "IMAGE_PATH = \"/content/car_plate_test.jpg\"\n",
        "IMAGE_SIZE = 640\n",
        "\n",
        "# Load the YOLOv8 model in ONNX format\n",
        "ort_session = ort.InferenceSession(MODEL_PATH)\n",
        "\n",
        "# Preprocess the input image for YOLOv8\n",
        "image = Image.open(IMAGE_PATH)  # Load the image using PIL\n",
        "scale_x = image.width / IMAGE_SIZE\n",
        "scale_y = image.height / IMAGE_SIZE\n",
        "resized_image = image.resize((IMAGE_SIZE, IMAGE_SIZE))  # Resize to 640x640\n",
        "\n",
        "# Convert PIL image to tensor\n",
        "transform = torchvision.transforms.ToTensor()\n",
        "input_tensor = transform(resized_image).unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Run the YOLOv8 model\n",
        "outputs = ort_session.run(None, {'images': input_tensor.numpy()})\n",
        "\n",
        "# Post-process the output tensor to get bounding box coordinates and confidence scores\n",
        "boxes = outputs[0][0]\n",
        "confidences = boxes[4]  # Confidence scores are stored in the 5th element\n",
        "\n",
        "# Find the bounding box with the highest confidence\n",
        "max_confidence_index = np.argmax(confidences)\n",
        "x, y, w, h, c = boxes[:, max_confidence_index]\n",
        "\n",
        "# Convert box format from [x_center, y_center, width, height] to [x_min, y_min, x_max, y_max]\n",
        "x_min, y_min = (x - w / 2) * scale_x, (y - h / 2) * scale_y\n",
        "x_max, y_max = (x + w / 2) * scale_x, (y + h / 2) * scale_y\n",
        "\n",
        "# Crop the license plate from the image\n",
        "license_plate_image = image.crop((x_min, y_min, x_max, y_max))\n",
        "\n",
        "# Use pytesseract to extract text\n",
        "license_plate_text = pytesseract.image_to_string(license_plate_image)\n",
        "print(license_plate_text)\n",
        "\n",
        "# Visualize the predicted bounding boxes on the image\n",
        "plt.imshow(image)\n",
        "current_axis = plt.gca()\n",
        "\n",
        "# Draw the bounding box and the license plate text on the image\n",
        "current_axis.add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, color='red', fill=False, linewidth=2))\n",
        "current_axis.text(x_min, y_min-10, f'License Plate: {license_plate_text} Confidence: {c:.2f}', bbox={'facecolor': 'white', 'alpha': 0.7})\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfk8uHrWtUXo"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import pytesseract\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "\n",
        "def get_detections(image_path, size, ort_session):\n",
        "    # Check if image_path is a string (indicating a file path)\n",
        "    if isinstance(image_path, str):\n",
        "        image = Image.open(image_path)\n",
        "    # Check if image_path is a NumPy array\n",
        "    elif isinstance(image_path, np.ndarray):\n",
        "        image = Image.fromarray(image_path)\n",
        "    else:\n",
        "        raise ValueError(\"image_path must be a file path (str) or a NumPy array.\")\n",
        "\n",
        "    scale_x = image.width / size\n",
        "    scale_y = image.height / size\n",
        "    resized_image = image.resize((size, size))\n",
        "    transform = torchvision.transforms.ToTensor()\n",
        "    input_tensor = transform(resized_image).unsqueeze(0)\n",
        "    outputs = ort_session.run(None, {'images': input_tensor.numpy()})\n",
        "    return image, outputs, scale_x, scale_y\n",
        "\n",
        "def non_maximum_supression(outputs):\n",
        "    boxes = outputs[0][0]\n",
        "    confidences = boxes[4]\n",
        "    max_confidence_index = np.argmax(confidences)\n",
        "    return boxes[:, max_confidence_index]\n",
        "\n",
        "def drawings(image, boxes, scale_x, scale_y):\n",
        "    x, y, w, h, c = boxes\n",
        "    x_min, y_min = (x - w / 2) * scale_x, (y - h / 2) * scale_y\n",
        "    x_max, y_max = (x + w / 2) * scale_x, (y + h / 2) * scale_y\n",
        "    license_plate_image = image.crop((x_min, y_min, x_max, y_max))\n",
        "    license_plate_text = pytesseract.image_to_string(license_plate_image)\n",
        "    print(license_plate_text)\n",
        "    image = cv2.rectangle(np.array(image), (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 0, 255), 3)\n",
        "    cv2.putText(image, f'License Plate: {license_plate_text}', (int(x_min), int(y_min-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "    cv2.putText(image, f'Confidence: {c:.2f}', (int(x_min), int(y_min+80)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "    return image\n",
        "\n",
        "def yolo_predictions(image_path, size, ort_session):\n",
        "    image, outputs, scale_x, scale_y = get_detections(image_path, size, ort_session)\n",
        "    boxes = non_maximum_supression(outputs)\n",
        "    result_img = drawings(image, boxes, scale_x, scale_y)\n",
        "    return result_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "r9UN01sFvgxt",
        "outputId": "030daea9-8fe5-4afc-bc83-9d1a23efa8e2"
      },
      "outputs": [],
      "source": [
        "  model_path = \"/content/drive/MyDrive/Licence_plate_detection_model/runs/detect/train2/weights/best.onnx\"\n",
        "  image_path = \"/content/car_plate_test.jpg\"\n",
        "  size = 640\n",
        "\n",
        "  ort_session = ort.InferenceSession(model_path)\n",
        "  result_img = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "    #cv2_imshow(result_img)\n",
        "    #cv2.waitKey(0)\n",
        "  fig = px.imshow(result_img)\n",
        "  fig.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "\n",
        "  fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "bwZgaK4auLy9",
        "outputId": "15252c2a-adb3-436d-eae6-2c0340d28175"
      },
      "outputs": [],
      "source": [
        "  image_path = \"/content/IMG_40872.28.29PM_1024x1024@2x.webp\"\n",
        "  size = 640\n",
        "\n",
        "  ort_session = ort.InferenceSession(model_path)\n",
        "  result_img = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "    #cv2_imshow(result_img)\n",
        "    #cv2.waitKey(0)\n",
        "  fig = px.imshow(result_img)\n",
        "  fig.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "R4c5Phb5uYI3",
        "outputId": "e8726241-4c86-4192-cbd3-6d5728d22a53"
      },
      "outputs": [],
      "source": [
        "  image_path = \"/content/lpr-tesla-license-plate-recognition-1910x1000.jpg\"\n",
        "  size = 640\n",
        "\n",
        "  ort_session = ort.InferenceSession(model_path)\n",
        "  result_img = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "    #cv2_imshow(result_img)\n",
        "    #cv2.waitKey(0)\n",
        "  fig = px.imshow(result_img)\n",
        "  fig.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtH_BafYupcG"
      },
      "source": [
        "# Cropped image processing\n",
        "\n",
        "Our model works great, however it seems like the OCR is strugguling to read out text from most of the images. Let's try and make the job easier for OCR by implementing some of the common image processing to the croped image in order to get better reading [source](https://stackoverflow.com/questions/9480013/image-processing-to-improve-tesseract-ocr-accuracy):\n",
        "\n",
        "*   **Rescale the image**\n",
        "*   **Convert to greyscale**\n",
        "*   **Remove the noise**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RJ2wUUYv-5v"
      },
      "outputs": [],
      "source": [
        "# Cropped image processing\n",
        "def ocr_image_process(img):\n",
        "  # Convert PIL Image to numpy array\n",
        "  img = np.array(img)\n",
        "  # Rescaling the image\n",
        "  img = cv2.resize(img, None, fx=1.2, fy=1.2, interpolation=cv2.INTER_CUBIC)\n",
        "  # Convert to grayscale\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  # Remove the noise\n",
        "  kernel = np.ones((1, 1), np.uint8)\n",
        "  img = cv2.dilate(img, kernel, iterations=1)\n",
        "  img = cv2.erode(img, kernel, iterations=1)\n",
        "  # Apply blur\n",
        "  #img = cv2.threshold(cv2.bilateralFilter(img, 5, 75, 75), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] # good\n",
        "\n",
        "  #img = cv2.threshold(cv2.medianBlur(img, 3), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] # better\n",
        "\n",
        "  #img = cv2.adaptiveThreshold(cv2.GaussianBlur(img, (5, 5), 0), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2) # good\n",
        "\n",
        "  img = cv2.adaptiveThreshold(cv2.bilateralFilter(img, 9, 75, 75), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2) # good\n",
        "\n",
        "  return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFX7lCSM1psj"
      },
      "outputs": [],
      "source": [
        "# Cropped image processing\n",
        "def ocr_image_process(img):\n",
        "  # Convert PIL Image to numpy array\n",
        "  img = np.array(img)\n",
        "\n",
        "  # Rescaling the image\n",
        "  #img = cv2.resize(img, None, fx=1.2, fy=1.2, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "  # Convert the image to grayscale\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Apply bilateral filter for noise reduction\n",
        "  bfilter = cv2.bilateralFilter(gray, 11, 17, 17)\n",
        "\n",
        "  # Apply edge detection\n",
        "  edged = cv2.Canny(bfilter, 30, 200)\n",
        "  #cv2_imshow(edged)\n",
        "\n",
        "  #cnts = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  #cnts = imutils.grab_contours(cnts)\n",
        "  #cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:10]\n",
        "  #screenCnt = None\n",
        "\n",
        "\n",
        "  #for c in cnts:\n",
        "  #  peri = cv2.arcLength(c, True)\n",
        "  #  approx = cv2.approxPolyDP(c, 0.018 * peri, True)\n",
        "  #  if len(approx) == 4:\n",
        "  #    screenCnt = approx\n",
        "  #    break\n",
        "  #mask = np.zeros(gray.shape, np.uint8)\n",
        "  #new_image = cv2.drawContours(mask,[screenCnt],0,255,-1)\n",
        "  #new_image = cv2.bitwise_and(img,img,mask=mask)\n",
        "\n",
        "  #(x, y) = np.where(mask == 255)\n",
        "  #(topx, topy) = (np.min(x), np.min(y))\n",
        "  #(bottomx, bottomy) = (np.max(x), np.max(y))\n",
        "  #cropped_img = gray[topx:bottomx+1, topy:bottomy+1]\n",
        "  #print(\"printing cropped img\")\n",
        "  #cv2_imshow(cropped_img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Find contours in the edged image\n",
        "  #keypoints = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  #contours = imutils.grab_contours(keypoints)\n",
        "\n",
        "  # Sort the contours based on the area and keep the top 10\n",
        "  #contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]\n",
        "\n",
        "  #location = None\n",
        "  # Loop over the contours to find a contour with 4 points (rectangle)\n",
        "  #for contour in contours:\n",
        "  #  approx = cv2.approxPolyDP(contour, 10, True)\n",
        "  #  if len(approx) == 4:\n",
        "  #      location = approx\n",
        "  #      break\n",
        "\n",
        "  #print(location)\n",
        "\n",
        "   #Create a mask to draw the contours\n",
        " # mask = np.zeros(gray.shape, np.uint8)\n",
        "\n",
        "  # Draw the contours on the mask\n",
        "  #new_image = cv2.drawContours(mask, [location], 0,255, -1)\n",
        "\n",
        "  # Bitwise-and the mask and the original image\n",
        "  #new_image = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "  # Find the coordinates of the area with the mask\n",
        "  #(x,y) = np.where(mask==255)\n",
        "  #(x1, y1) = (np.min(x), np.min(y))\n",
        "  #(x2, y2) = (np.max(x), np.max(y))\n",
        "\n",
        "  # Crop the original image\n",
        "  #cropped_image = gray[x1:x2+1, y1:y2+1]\n",
        "\n",
        "  # Return the cropped image\n",
        "\n",
        "  return bfilter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAGY9DsV3SEW"
      },
      "source": [
        "## Visualize cropped image before and after processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZuguuJKv-0D"
      },
      "outputs": [],
      "source": [
        "# Now lets try again but let's visualize the cropped image before and after processing therefore we have to make some changes to the original functions\n",
        "import cv2\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import pytesseract\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "\n",
        "def get_detections(image_path, size, ort_session):\n",
        "    # Check if image_path is a string (indicating a file path)\n",
        "    if isinstance(image_path, str):\n",
        "        # Check if the image is a PNG\n",
        "        if image_path.lower().endswith('.png'):\n",
        "            # Open the image file\n",
        "            img = Image.open(image_path)\n",
        "            # Convert the image to RGB (removes the alpha channel)\n",
        "            rgb_img = img.convert('RGB')\n",
        "            # Create a new file name by replacing .png with .jpg\n",
        "            jpg_image_path = os.path.splitext(image_path)[0] + '.jpg'\n",
        "            # Save the RGB image as a JPG\n",
        "            rgb_img.save(jpg_image_path)\n",
        "            # Update image_path to point to the new JPG image\n",
        "            image_path = jpg_image_path\n",
        "\n",
        "        image = Image.open(image_path)\n",
        "    # Check if image_path is a NumPy array\n",
        "    elif isinstance(image_path, np.ndarray):\n",
        "        image = Image.fromarray(image_path)\n",
        "    else:\n",
        "        raise ValueError(\"image_path must be a file path (str) or a NumPy array.\")\n",
        "\n",
        "    scale_x = image.width / size\n",
        "    scale_y = image.height / size\n",
        "    resized_image = image.resize((size, size))\n",
        "    transform = torchvision.transforms.ToTensor()\n",
        "    input_tensor = transform(resized_image).unsqueeze(0)\n",
        "    outputs = ort_session.run(None, {'images': input_tensor.numpy()})\n",
        "    return image, outputs, scale_x, scale_y\n",
        "\n",
        "def non_maximum_supression(outputs):\n",
        "    boxes = outputs[0][0]\n",
        "    confidences = boxes[4]\n",
        "    max_confidence_index = np.argmax(confidences)\n",
        "    return boxes[:, max_confidence_index]\n",
        "\n",
        "def drawings(image, boxes, scale_x, scale_y):\n",
        "    x, y, w, h, c = boxes\n",
        "    x_min, y_min = (x - w / 2) * scale_x, (y - h / 2) * scale_y\n",
        "    x_max, y_max = (x + w / 2) * scale_x, (y + h / 2) * scale_y\n",
        "    license_plate_image = image.crop((x_min, y_min, x_max, y_max))\n",
        "    processed_cropped_image = ocr_image_process(license_plate_image)\n",
        "    license_plate_text = pytesseract.image_to_string(processed_cropped_image)\n",
        "    print(license_plate_text)\n",
        "    image = cv2.rectangle(np.array(image), (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 0, 255), 3)\n",
        "    cv2.putText(image, f'License Plate: {license_plate_text}', (int(x_min), int(y_min-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "    cv2.putText(image, f'Confidence: {c:.2f}', (int(x_min), int(y_min+80)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "    return image, license_plate_image, processed_cropped_image\n",
        "\n",
        "def yolo_predictions(image_path, size, ort_session):\n",
        "    image, outputs, scale_x, scale_y = get_detections(image_path, size, ort_session)\n",
        "    boxes = non_maximum_supression(outputs)\n",
        "    result_img, license_plate_image, processed_cropped_image = drawings(image, boxes, scale_x, scale_y)\n",
        "    return result_img, license_plate_image, processed_cropped_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UQfY7krcv-xo",
        "outputId": "3448be40-2295-4006-99df-ef15080fa305"
      },
      "outputs": [],
      "source": [
        "  image_path = \"/content/lpr-tesla-license-plate-recognition-1910x1000.jpg\"\n",
        "  size = 640\n",
        "\n",
        "  ort_session = ort.InferenceSession(model_path)\n",
        "  result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "    #cv2_imshow(result_img)\n",
        "    #cv2.waitKey(0)\n",
        "  # Plot for result_img\n",
        "  fig1 = px.imshow(result_img)\n",
        "  fig1.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig1.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig1.show()\n",
        "\n",
        "  # Plot for license_plate_image\n",
        "  fig2 = px.imshow(license_plate_image)\n",
        "  fig2.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig2.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig2.show()\n",
        "\n",
        "  # Plot for processed_cropped_image\n",
        "  fig3 = px.imshow(processed_cropped_image)\n",
        "  fig3.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig3.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KUi-vDshv-vP",
        "outputId": "a5357d19-fb5c-4a49-ad29-3114b7e7fcf0"
      },
      "outputs": [],
      "source": [
        "  image_path = \"/content/Car+Number+Plate+-+MP+3894.jpg\"\n",
        "  size = 640\n",
        "\n",
        "  ort_session = ort.InferenceSession(model_path)\n",
        "  result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "    #cv2_imshow(result_img)\n",
        "    #cv2.waitKey(0)\n",
        "  # Plot for result_img\n",
        "  fig1 = px.imshow(result_img)\n",
        "  fig1.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig1.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig1.show()\n",
        "\n",
        "  # Plot for license_plate_image\n",
        "  fig2 = px.imshow(license_plate_image)\n",
        "  fig2.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig2.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig2.show()\n",
        "\n",
        "  # Plot for processed_cropped_image\n",
        "  fig3 = px.imshow(processed_cropped_image)\n",
        "  fig3.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig3.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eElzBW8Wv-st",
        "outputId": "9d5648b3-b513-42f6-b32e-5c2756cf0bca"
      },
      "outputs": [],
      "source": [
        "  image_path = \"/content/Front-Number-Plate.webp\"\n",
        "  size = 640\n",
        "\n",
        "  ort_session = ort.InferenceSession(model_path)\n",
        "  result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "    #cv2_imshow(result_img)\n",
        "    #cv2.waitKey(0)\n",
        "  # Plot for result_img\n",
        "  fig1 = px.imshow(result_img)\n",
        "  fig1.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig1.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig1.show()\n",
        "\n",
        "  # Plot for license_plate_image\n",
        "  fig2 = px.imshow(license_plate_image)\n",
        "  fig2.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig2.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig2.show()\n",
        "\n",
        "  # Plot for processed_cropped_image\n",
        "  fig3 = px.imshow(processed_cropped_image)\n",
        "  fig3.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig3.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig3.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmhowBuc35gO"
      },
      "source": [
        "Implement EasyOCR functionality and provide the funcionality to choose between PyTesseract and EeasyOCR for text recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxXV5wSyv-gz"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import imutils\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def get_detections(image_path, size, ort_session):\n",
        "    \"\"\"\n",
        "    Function to get detections from the model.\n",
        "    \"\"\"\n",
        "    # Check if image_path is a string (indicating a file path)\n",
        "    if isinstance(image_path, str):\n",
        "        # Check if the image is a PNG\n",
        "        if image_path.lower().endswith('.png'):\n",
        "            # Open the image file\n",
        "            img = Image.open(image_path)\n",
        "            # Convert the image to RGB (removes the alpha channel)\n",
        "            rgb_img = img.convert('RGB')\n",
        "            # Create a new file name by replacing .png with .jpg\n",
        "            jpg_image_path = os.path.splitext(image_path)[0] + '.jpg'\n",
        "            # Save the RGB image as a JPG\n",
        "            rgb_img.save(jpg_image_path)\n",
        "            # Update image_path to point to the new JPG image\n",
        "            image_path = jpg_image_path\n",
        "\n",
        "        image = Image.open(image_path)\n",
        "    # Check if image_path is a NumPy array\n",
        "    elif isinstance(image_path, np.ndarray):\n",
        "        image = Image.fromarray(image_path)\n",
        "    else:\n",
        "        raise ValueError(\"image_path must be a file path (str) or a NumPy array.\")\n",
        "\n",
        "    scale_x = image.width / size\n",
        "    scale_y = image.height / size\n",
        "    resized_image = image.resize((size, size))\n",
        "    transform = torchvision.transforms.ToTensor()\n",
        "    input_tensor = transform(resized_image).unsqueeze(0)\n",
        "    outputs = ort_session.run(None, {'images': input_tensor.numpy()})\n",
        "    return image, outputs, scale_x, scale_y\n",
        "\n",
        "def non_maximum_supression(outputs):\n",
        "    \"\"\"\n",
        "    Function to apply non-maximum suppression.\n",
        "    \"\"\"\n",
        "    boxes = outputs[0][0]\n",
        "    confidences = boxes[4]\n",
        "    max_confidence_index = np.argmax(confidences)\n",
        "    return boxes[:, max_confidence_index]\n",
        "\n",
        "def drawings(image, boxes, scale_x, scale_y, ocr=\"pytesseract\"):\n",
        "    \"\"\"\n",
        "    Function to draw bounding boxes and apply OCR.\n",
        "    \"\"\"\n",
        "    x, y, w, h, c = boxes\n",
        "    x_min, y_min = (x - w / 2) * scale_x, (y - h / 2) * scale_y\n",
        "    x_max, y_max = (x + w / 2) * scale_x, (y + h / 2) * scale_y\n",
        "    license_plate_image = image.crop((x_min, y_min, x_max, y_max))\n",
        "    processed_cropped_image = ocr_image_process(license_plate_image)\n",
        "\n",
        "    if ocr == \"ez\":\n",
        "      import easyocr\n",
        "      reader = easyocr.Reader(['en'])\n",
        "      result = reader.readtext(processed_cropped_image)\n",
        "      license_plate_text = str.upper(result[0][1])\n",
        "      print(license_plate_text)\n",
        "    else:\n",
        "      import pytesseract\n",
        "      license_plate_text = pytesseract.image_to_string(processed_cropped_image)\n",
        "      print(license_plate_text)\n",
        "\n",
        "    image = cv2.rectangle(np.array(image), (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 0, 255), 3)\n",
        "    cv2.putText(image, f'License Plate: {license_plate_text}', (int(x_min), int(y_min-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "    cv2.putText(image, f'Confidence: {c:.2f}', (int(x_min), int(y_min+80)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
        "    return image, license_plate_image, processed_cropped_image\n",
        "\n",
        "def yolo_predictions(image_path, size, ort_session, ocr=\"pytesseract\"):\n",
        "    \"\"\"\n",
        "    Function to get YOLO predictions.\n",
        "    \"\"\"\n",
        "    image, outputs, scale_x, scale_y = get_detections(image_path, size, ort_session)\n",
        "    boxes = non_maximum_supression(outputs)\n",
        "    result_img, license_plate_image, processed_cropped_image = drawings(image, boxes, scale_x, scale_y, ocr)\n",
        "    return result_img, license_plate_image, processed_cropped_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QtWL57G7v-WM",
        "outputId": "ee0539a4-363d-4834-8bb1-d57a25729147"
      },
      "outputs": [],
      "source": [
        "  # Test the EasyOCR\n",
        "  image_path = \"/content/lpr-tesla-license-plate-recognition-1910x1000.jpg\"\n",
        "  size = 640\n",
        "\n",
        "  ort_session = ort.InferenceSession(model_path)\n",
        "  result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session, \"ez\")\n",
        "\n",
        "    #cv2_imshow(result_img)\n",
        "    #cv2.waitKey(0)\n",
        "  # Plot for result_img\n",
        "  fig1 = px.imshow(result_img)\n",
        "  fig1.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig1.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig1.show()\n",
        "\n",
        "  # Plot for license_plate_image\n",
        "  fig2 = px.imshow(license_plate_image)\n",
        "  fig2.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig2.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig2.show()\n",
        "\n",
        "  # Plot for processed_cropped_image\n",
        "  fig3 = px.imshow(processed_cropped_image)\n",
        "  fig3.update_layout(width=700, height=400, margin=dict(l=10, r=10, b=10, t=10))\n",
        "  fig3.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "  fig3.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX1WKq94ZKEe"
      },
      "source": [
        "## Process and visualize predictions for multiple images from a directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "PkcQHO4ogQeJ",
        "outputId": "6d4f156c-0e94-49b9-ce8c-ac62d0116b9f"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "\n",
        "# Specify the directory path\n",
        "dir_path = \"/content/\"\n",
        "\n",
        "size=640\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "image_files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
        "\n",
        "# Loop through all the image files\n",
        "for image_file in image_files:\n",
        "    # Construct the full image path\n",
        "    image_path = os.path.join(dir_path, image_file)\n",
        "\n",
        "    # Your existing code here\n",
        "    ort_session = ort.InferenceSession(model_path)\n",
        "    result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "    license_plate_image_resized = cv2.resize(np.array(license_plate_image), (result_img.shape[1], result_img.shape[0]//2))\n",
        "    processed_cropped_image_resized = cv2.resize(np.array(processed_cropped_image), (result_img.shape[1], result_img.shape[0]//2))\n",
        "\n",
        "    processed_cropped_image_resized_rgb = cv2.cvtColor(processed_cropped_image_resized, cv2.COLOR_GRAY2RGB)\n",
        "    right_image = np.vstack((license_plate_image_resized, processed_cropped_image_resized_rgb))\n",
        "\n",
        "    if right_image.shape[0] > result_img.shape[0]:\n",
        "        result_img = cv2.resize(result_img, (result_img.shape[1], right_image.shape[0]))\n",
        "    else:\n",
        "        right_image = cv2.resize(right_image, (right_image.shape[1], result_img.shape[0]))\n",
        "\n",
        "    combined_image = np.hstack((result_img, right_image))\n",
        "\n",
        "    fig = px.imshow(combined_image)\n",
        "    fig.update_layout(width=700, height=400)\n",
        "    fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "    fig.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe86QqnX4uEd"
      },
      "source": [
        "### Test the PyTesseract on multple images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fX1CDJzvZHUN",
        "outputId": "3c546d97-1846-41ca-876d-d09ab39e7f30"
      },
      "outputs": [],
      "source": [
        "ort_session = ort.InferenceSession(model_path)\n",
        "result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "\n",
        "# Specify the directory path\n",
        "dir_path = \"/content/\"\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "image_files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
        "\n",
        "# Loop through all the image files\n",
        "for image_file in image_files:\n",
        "    # Construct the full image path\n",
        "    image_path = os.path.join(dir_path, image_file)\n",
        "\n",
        "    # Your existing code here\n",
        "    ort_session = ort.InferenceSession(model_path)\n",
        "    result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "    license_plate_image_resized = cv2.resize(np.array(license_plate_image), (result_img.shape[1], result_img.shape[0]//2))\n",
        "    processed_cropped_image_resized = cv2.resize(np.array(processed_cropped_image), (result_img.shape[1], result_img.shape[0]//2))\n",
        "\n",
        "    processed_cropped_image_resized_rgb = cv2.cvtColor(processed_cropped_image_resized, cv2.COLOR_GRAY2RGB)\n",
        "    right_image = np.vstack((license_plate_image_resized, processed_cropped_image_resized_rgb))\n",
        "\n",
        "    if right_image.shape[0] > result_img.shape[0]:\n",
        "        result_img = cv2.resize(result_img, (result_img.shape[1], right_image.shape[0]))\n",
        "    else:\n",
        "        right_image = cv2.resize(right_image, (right_image.shape[1], result_img.shape[0]))\n",
        "\n",
        "    combined_image = np.hstack((result_img, right_image))\n",
        "\n",
        "    fig = px.imshow(combined_image)\n",
        "    fig.update_layout(width=700, height=400)\n",
        "    fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqWZfBPC5FbX"
      },
      "source": [
        "### Test the EasyOCR on multple images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oTkvH6pNpOe_",
        "outputId": "4bbd7655-ddb4-48cc-f5b9-1c2174636ce0"
      },
      "outputs": [],
      "source": [
        "size = 640\n",
        "\n",
        "#ort_session = ort.InferenceSession(model_path)\n",
        "#result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "\n",
        "# Specify the directory path\n",
        "dir_path = \"/content/\"\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "image_files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
        "\n",
        "# Loop through all the image files\n",
        "for image_file in image_files:\n",
        "    # Construct the full image path\n",
        "    image_path = os.path.join(dir_path, image_file)\n",
        "\n",
        "    # Your existing code here\n",
        "    ort_session = ort.InferenceSession(model_path)\n",
        "    result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session, \"ez\")\n",
        "\n",
        "    license_plate_image_resized = cv2.resize(np.array(license_plate_image), (result_img.shape[1], result_img.shape[0]//2))\n",
        "    processed_cropped_image_resized = cv2.resize(np.array(processed_cropped_image), (result_img.shape[1], result_img.shape[0]//2))\n",
        "\n",
        "    processed_cropped_image_resized_rgb = cv2.cvtColor(processed_cropped_image_resized, cv2.COLOR_GRAY2RGB)\n",
        "    right_image = np.vstack((license_plate_image_resized, processed_cropped_image_resized_rgb))\n",
        "\n",
        "    if right_image.shape[0] > result_img.shape[0]:\n",
        "        result_img = cv2.resize(result_img, (result_img.shape[1], right_image.shape[0]))\n",
        "    else:\n",
        "        right_image = cv2.resize(right_image, (right_image.shape[1], result_img.shape[0]))\n",
        "\n",
        "    combined_image = np.hstack((result_img, right_image))\n",
        "\n",
        "    fig = px.imshow(combined_image)\n",
        "    fig.update_layout(width=700, height=400)\n",
        "    fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOMcGC-z5-Kl"
      },
      "source": [
        "## Adaptive threshholding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "_xZaBolqFx7k",
        "outputId": "e64d3774-171b-4821-9576-f54213a2a66e"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "image_path = \"/content/test_plate.png\"\n",
        " # Check if the image is a PNG\n",
        "if image_path.lower().endswith('.png'):\n",
        "           # Open the image file\n",
        "  img = Image.open(image_path)\n",
        "            # Convert the image to RGB (removes the alpha channel)\n",
        "  rgb_img = img.convert('RGB')\n",
        "            # Create a new file name by replacing .png with .jpg\n",
        "  jpg_image_path = os.path.splitext(image_path)[0] + '.jpg'\n",
        "            # Save the RGB image as a JPG\n",
        "  rgb_img.save(jpg_image_path)\n",
        "            # Update image_path to point to the new JPG image\n",
        "  image_path = jpg_image_path\n",
        "\n",
        "  image = Image.open(image_path)\n",
        "\n",
        "  image = np.array(image)\n",
        "\n",
        "\n",
        "# Calculate skew angle of an image\n",
        "#img = cv2.imread('/content/test_plate.png', 0)\n",
        "  # Prep image, copy, convert to gray scale, blur, and threshold\n",
        "newImage = image.copy()\n",
        "gray = cv2.cvtColor(newImage, cv2.COLOR_BGR2GRAY)\n",
        "# Resize the image\n",
        "#image = cv2.resize(image, None, fx=1.2, fy=1.2, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# Convert to grayscale\n",
        "#gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply adaptive thresholding\n",
        "thresh = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,25)\n",
        "\n",
        "# Perform morphological operations\n",
        "kernel = np.ones((5,5),np.uint8)\n",
        "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "return opening\n",
        "\n",
        "# Apply distance transform\n",
        "#dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
        "#ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
        "\n",
        "#cv2_imshow(opening)\n",
        "#fig = px.imshow(minAreaRect)\n",
        "#fig.update_layout(width=700, height=400)\n",
        "#fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "#fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB0LkUjLTR_5"
      },
      "outputs": [],
      "source": [
        "# Cropped image processing\n",
        "def ocr_image_process(img):\n",
        "  # Convert PIL Image to numpy array\n",
        "  img = np.array(img)\n",
        "\n",
        "  # Rescaling the image\n",
        "  #img = cv2.resize(img, None, fx=1.2, fy=1.2, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "  newImage = img.copy()\n",
        "  gray = cv2.cvtColor(newImage, cv2.COLOR_BGR2GRAY)\n",
        "# Resize the image\n",
        "#image = cv2.resize(image, None, fx=1.2, fy=1.2, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# Convert to grayscale\n",
        "#gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply adaptive thresholding\n",
        "  thresh = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,25)\n",
        "\n",
        "# Perform morphological operations\n",
        "  kernel = np.ones((5,5),np.uint8)\n",
        "  opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "  return thresh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m3I_V4OHT0kj",
        "outputId": "126494d0-7fa8-4cd5-b624-0d3bff115140"
      },
      "outputs": [],
      "source": [
        "size = 640\n",
        "model_path = \"/content/drive/MyDrive/Licence_plate_detection_model/runs/detect/train2/weights/best.onnx\"\n",
        "\n",
        "#ort_session = ort.InferenceSession(model_path)\n",
        "#result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "\n",
        "# Specify the directory path\n",
        "dir_path = \"/content/\"\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "image_files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
        "\n",
        "# Loop through all the image files\n",
        "for image_file in image_files:\n",
        "    # Construct the full image path\n",
        "    image_path = os.path.join(dir_path, image_file)\n",
        "\n",
        "    # Your existing code here\n",
        "    ort_session = ort.InferenceSession(model_path)\n",
        "    result_img, license_plate_image, processed_cropped_image = yolo_predictions(image_path, size, ort_session)\n",
        "\n",
        "    license_plate_image_resized = cv2.resize(np.array(license_plate_image), (result_img.shape[1], result_img.shape[0]//2))\n",
        "    processed_cropped_image_resized = cv2.resize(np.array(processed_cropped_image), (result_img.shape[1], result_img.shape[0]//2))\n",
        "\n",
        "    processed_cropped_image_resized_rgb = cv2.cvtColor(processed_cropped_image_resized, cv2.COLOR_GRAY2RGB)\n",
        "    right_image = np.vstack((license_plate_image_resized, processed_cropped_image_resized_rgb))\n",
        "\n",
        "    if right_image.shape[0] > result_img.shape[0]:\n",
        "        result_img = cv2.resize(result_img, (result_img.shape[1], right_image.shape[0]))\n",
        "    else:\n",
        "        right_image = cv2.resize(right_image, (right_image.shape[1], result_img.shape[0]))\n",
        "\n",
        "    combined_image = np.hstack((result_img, right_image))\n",
        "\n",
        "    fig = px.imshow(combined_image)\n",
        "    fig.update_layout(width=700, height=400)\n",
        "    fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "    fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
